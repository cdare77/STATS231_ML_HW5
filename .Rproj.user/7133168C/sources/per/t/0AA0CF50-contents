---
title: "STATS231_Dare_HW2"
author: Chris Dare
output: html_document
date: "2024-04-21"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(corrplot)
library(ggthemes)
library(kableExtra)
library(yardstick)
tidymodels_prefer()
conflicted::conflicts_prefer(yardstick::rsq)

set.seed(3435)
```

```{r eval = TRUE}
ABAL <- read.csv("abalone.csv")
head(ABAL, 5)
tail(ABAL, 5)
```


## Question 1

Your goal is to predict abalone age, which is calculated as the number of rings plus 1.5. Notice there currently is no age variable in the data set. Add age to the data set.

Assess and describe the distribution of age.


```{r eval = TRUE, include=TRUE, warnings=FALSE}
ABAL2 <- transform(ABAL, age=rings+1.5)

ggplot(ABAL2, aes(x=age))+
  geom_histogram(color="darkblue", fill="lightblue", binwidth = 1)
```

The vast majority of abalone are aged 9-11 years old, with the number of abalone living past 13 years exponentially dropping off


## Question 2

Split the abalone data into a training set and a testing set. Use stratified sampling. You should decide on appropriate percentages for splitting the data.



```{r eval = TRUE, include=TRUE, warnings=FALSE}
abal_split <- initial_split(ABAL2, prop = 3/4,
                                strata = age)
abal_train <- training(abal_split)
abal_test <- testing(abal_split)
```


## Question 3 

Using the training data, create a recipe predicting the outcome variable, age, with all other predictor variables. Note that you should not include rings to predict age. Explain why you shouldn’t use rings to predict age.



```{r eval = TRUE, include=TRUE, warnings=FALSE}
abalone_recipe <-
  recipe(age ~ type + longest_shell + diameter + height + whole_weight + shucked_weight + viscera_weight + shell_weight, data = abal_train)

int_mod_1 <- abalone_recipe %>%
  step_dummy(all_nominal_predictors())


  
int_mod_2 <- int_mod_1 %>% 
  step_interact(terms= ~ starts_with("type"):shucked_weight) %>%
  step_interact(terms= ~ longest_shell:diameter) %>%
  step_interact(terms= ~ shucked_weight:shell_weight)

centered_scaled_data <- int_mod_2 %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())
```

*Rings should not be used to predict age since there is already an explicit relation in the data between age and rings; specifically age = rings + 1.5. This will just increase the possibility of over-fitting the data.*


## Question 4

Create and store a linear regression object using the "lm" engine.

```{r eval = TRUE, include=TRUE, warnings=FALSE}
 lm_model <- linear_reg() %>% 
  set_engine("lm")
```


## Question 5

Create and store a KNN object using the "kknn" engine. Specify k = 7.

```{r eval = TRUE, include=TRUE, warnings=FALSE}
knn_model <- nearest_neighbor(neighbors = 7) %>% 
  set_engine("kknn") %>% 
  set_mode("regression")
```


## Question 6 

Now, for each of these models (linear regression and KNN):

1. set up an empty workflow,
2. add the model, and
3. add the recipe that you created in Question 3.

Note that you should be setting up two separate workflows.

Fit both models to the training set.

```{r eval = TRUE, include=TRUE, warnings=FALSE}
lm_wflow <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(centered_scaled_data)

lm_fit <- fit(lm_wflow, abal_train)
```

```{r eval = TRUE, include=TRUE, warnings=FALSE}
 knn_wflow <- workflow() %>% 
  add_model(knn_model) %>% 
  add_recipe(centered_scaled_data)

knn_fit <- fit(knn_wflow, abal_train)
```


## Question 7

Use your linear regression fit() object to predict the age of a hypothetical female abalone with longest_shell = 0.50, diameter = 0.10, height = 0.30, whole_weight = 4, shucked_weight = 1, viscera_weight = 2, and shell_weight = 1.

```{r eval = TRUE, include=TRUE, warnings=FALSE}
newdata = data.frame(type='F', longest_shell = 0.50, diameter = 0.10, height = 0.30, whole_weight = 4, shucked_weight = 1, viscera_weight = 2, shell_weight = 1)

predict(lm_fit, new_data = newdata)
```


## Question 8  

Now you want to assess your models’ performance. To do this, use the `yardstick` package:

1. Create a metric set that includes $R^2$, RMSE (root mean squared error), and MAE (mean absolute error).
2. Use `augment()`  to create a tibble of your model’s predicted values from the **testing data** along with the actual observed ages (these are needed to assess your model’s performance).
3. Finally, apply your metric set to the tibble, report the results, and interpret the $R^2$ value.

Repeat these steps once for the linear regression model and for the KNN model.


```{r eval = TRUE, include=TRUE, warnings=FALSE}
 my_metrics <- metric_set(rmse, mae, rsq)


 lm_augmented_data <- augment(lm_fit, new_data=abal_test)
 my_metrics(lm_augmented_data, truth = age, estimate=.pred)
```

```{r eval = TRUE, include=TRUE, warnings=FALSE}
 knn_augmented_data <- augment(knn_fit, new_data=abal_test)
 my_metrics(knn_augmented_data, truth = age, estimate=.pred)
```


*As the* $R^2$ *value in the linear regression model is slightly higher, the variation in the testing data is better explained by the linear regression model than the k-nearest neighbors model. In particular, 54% of the variation in abalone age is described by the linear regression model while only 46% of the variation is explained by the k-nearest neighbors model. *



## Question 9 

Which model performed better on the testing data? Explain why you think this might be. Are you surprised by any of your results? Why or why not?


*The linear regression model was a better fit for the data since there is a lower mean squared error and the $R^2$ value is closer to 1. Ultimately, this could either because the number of neighbors (k-value) is not the optimal choice or because the actual relation between age and the other predictors follows a linear relationship. *

*The fact that linear regression was a better fit is not surprising since one would generally expect that abalone grow as they age — that is, we expect that increases in age would lead to increases in weight, height, and diameter. Therefore we are assuming the data is well approximated by a global linear function instead of approximated by a locally constant function.*


## Question 10

Which term(s) in the bias-variance tradeoff above represent the reducible error? Which term(s) represent the irreducible error?

*The terms $\text{Var}(\hat{f}(x_0))$ and $[\text{Bias}(\hat{f}(x_0)]^2$ represent the reducible error. The $\text{Var}(\epsilon)$ represents irriducible error.*

## Question 11

Using the bias-variance tradeoff above, demonstrate that the expected test error is always at least as large as the irreducible error.

*By the bias-variance tradeoff,* $E[ (y_0 - \hat{f}(x_0))^2 ] = \text{Var}(\hat{f}(x_0)) +  [\text{Bias}(\hat{f}(x_0)]^2 + \text{Var}(\epsilon)$ *where the irreducible error is given by * $\text{Var}(\epsilon)$. *To show that* $E[ (y_0 - \hat{f}(x_0) )^2 ] \geq \text{Var}(\epsilon)$, *it suffices to show that the reducible error is non-negative; that is,* $\text{Var}(\hat{f}(x_0)) +  [\text{Bias}(\hat{f}(x_0)]^2 \geq 0$. *Since the square of any real number is non-negative,* $[\text{Bias}(\hat{f}(x_0)]^2 \geq 0$ *always holds — therefore we may reduce the problem to showing that* $\text{Var}(\hat{f}(x_0))$. *However, this follows from the fact that variance is the expectation of a non-negative random variable:* $\text{Var}(\hat{f}(x_0)) := E[ ( \hat{f}(x_0) - E\hat{f}(x_0) )^2 ]$.


## Question 12

Prove the bias-variance tradeoff.


*Since we are assuming the underlying model satisfies* $Y = f + \epsilon$ *with* $E[\epsilon] = 0$, *we have* 
\[ 
\begin{align}
E[(Y - \hat{f} )^2] &= E[ (f  + \epsilon - \hat{f} )^2 ] = E[ (f - \hat{f} )^2] - 2E[(f  - \hat{f} )\epsilon ] + E[\epsilon^2]
\\&= E[ (f  - \hat{f} )^2   ] - 2f E[\epsilon] - 2E[\hat{f}]E[\epsilon] + E[\epsilon^2]
\\&= E[ (f - \hat{f})^2] + E[\epsilon^2]
\end{align}
\]
*where we use the fact that* $E[f] = f$ *and that* $\hat{f}$ *and* $\epsilon$ *are independent. Now we may expand * $E[ (f - \hat{f})^2 ]$ *as*
\[
\begin{align}
E[(f - \hat{f})^2] &= E[ (f - E[\hat{f}]) + E[\hat{f}] - \hat{f})^2 ]
\\&= E[f - E[\hat{f}]]^2 + E[ \hat{f} - E[\hat{f}]  ]^2
\\&= [\operatorname{Bias}[\hat{f}]]^2 + \operatorname{Var}(\hat{f})
\end{align}
\]
*Since* $\operatorname{Var}(\epsilon) = E[(\epsilon - E(\epsilon))^2] = E[\epsilon^2]$, *the result follows.*